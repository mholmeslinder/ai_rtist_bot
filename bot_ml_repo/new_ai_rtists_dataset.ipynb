{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Dataset creation for new_ai_rtists Twitter Bot\n",
    "\n",
    "### First, we use the [Last.fm API](https://www.last.fm/api/) to create our dataset\n",
    "\n",
    "#### This was inspired by/heavily cribbed from [this great tutorial](https://www.dataquest.io/blog/last-fm-api-python/) on Dataquest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes that you have either done that tutorial or at least understood most of the different elements that go into it. I'll try to elucidate every time I do something that's different from the DQ tutorial.\n",
    "\n",
    "### Note:\n",
    "This demonstration is recreating the methodology I used to create our *artist_names.txt* dataset, but since that methodology involves accessing ever-changing data from an API that tracks ever-evolving real-life events, these operations will always spit out slightly different datasets. That's part of the fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import requests_cache\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "\n",
    "requests_cache.install_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll be storing our Last.fm API info as environment variables. There are many ways to do this - I'm using python-dotenv and a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "USER_AGENT = os.getenv('USER_AGENT')\n",
    "API_KEY = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lastfm_get(payload):\n",
    "    # define headers and URL\n",
    "    headers = {'user-agent': USER_AGENT}\n",
    "    url = 'http://ws.audioscrobbler.com/2.0/'\n",
    "\n",
    "    # Add API key and format to the payload\n",
    "    payload['api_key'] = API_KEY\n",
    "    payload['format'] = 'json'\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=payload)\n",
    "    return response\n",
    "\n",
    "def jprint(obj):\n",
    "    # create a formatted string of the Python JSON object\n",
    "    text = json.dumps(obj, sort_keys=True, indent=4)\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the tutorial, we'll start by calling the *chart.getTopArtists* endpoint for every single \"artists\" key and end up with a massive dump of json info. \n",
    "#### NOTE: This dump is actually much smaller than it initially appears, due to an undocumented limitation of the API. \n",
    "\n",
    "Basically, after the first twenty responses, the API doesn't return any data. So, instead of having a list of (500 x ~6,000) results, we have only about 10,000. \n",
    "\n",
    "This is a huge part of why, in Part 2, we'll be using **GPT-2** as our network of choice. If we can figure out a way of accessing more Last.fm API results and getting a bigger dataset, it would be a very interesting followup to use that to train a 'from-scratch' text-generation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting page 6205/6205\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "\n",
    "page = 1\n",
    "total_pages = 99999 # this is just a dummy number so the loop starts\n",
    "\n",
    "while page <= total_pages:\n",
    "    payload = {\n",
    "        'method': 'chart.gettopartists',\n",
    "        'limit': 500,\n",
    "        'page': page\n",
    "    }\n",
    "\n",
    "    # print some output so we can see the status\n",
    "    print(\"Requesting page {}/{}\".format(page, total_pages))\n",
    "    # clear the output to make things neater\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    # make the API call\n",
    "    response = lastfm_get(payload)\n",
    "\n",
    "    # if we get an error, print the response and halt the loop\n",
    "    if response.status_code != 200:\n",
    "        print(response.text)\n",
    "        break\n",
    "\n",
    "    # extract pagination info\n",
    "    page = int(response.json()['artists']['@attr']['page'])\n",
    "    total_pages = int(response.json()['artists']['@attr']['totalPages'])\n",
    "\n",
    "    # append response\n",
    "    responses.append(response)\n",
    "\n",
    "    # if it's not a cached result, sleep\n",
    "    if not getattr(response, 'from_cache', False):\n",
    "        time.sleep(0.25)\n",
    "\n",
    "    # increment the page number\n",
    "    page += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then use Pandas to turn this into a list of dataframes, which will make it MUCH easier to acccess the specific features we want. At this point, we can also drop the features we know we aren't interested in and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [pd.DataFrame(r.json()['artists']['artist']) for r in responses]\n",
    "artists = pd.concat(frames)\n",
    "artists = artists[['name']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just getting artist names series, then converting to list via dict, so we can remove the duplicates\n",
    "artist_names = artists[\"name\"]\n",
    "names_list = list(dict.fromkeys(artist_names.tolist()))\n",
    "\n",
    "# Convering to string\n",
    "names_string = \"\\n\".join([str(x) for x in names_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're ready to write our dataset to a .txt file for use with **GPT-2**. That was easy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./artist_names.txt\", \"w\") as file:\n",
    "    file.write(names_string)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "\n",
    "This is truly version 0 of this project, a proof of concept to see if the GPT-2 results were inspiring (spoiler: they totally are!). Future versions are going to take advantage of some other Last.fm API endpoints, like *artist.getTopTags*, to create 'genres' for our fictional artists. \n",
    "\n",
    "From there, we can implement some cross-functionality with the [Genius API](https://docs.genius.com/) to create large datasets of lyrics from our AI-generated genres, and I am SURE GPT-2 would be happy to help us come up with some lyrics for our fictional artists..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
